{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a2b39a3",
   "metadata": {},
   "source": [
    "## Use Case\n",
    "### Performance Comparison of Reading a Large CSV File with and without Chunking\n",
    "\n",
    "In this demonstration, we will:\n",
    "\n",
    "1. Simulate the creation of a large CSV file.\n",
    "2. Read the CSV file into a Pandas DataFrame in two different ways:\n",
    "    - Without chunking\n",
    "    - With chunking\n",
    "3. Compare the performance of both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84843a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "103e1683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from memory_profiler import memory_usage\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a710e4ab",
   "metadata": {},
   "source": [
    "### Step 1: Simulating a Large CSV File\n",
    "\n",
    "First, let's create a large CSV file for testing purposes. We will generate a DataFrame with a significant number of rows and save it as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e91a09fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a large DataFrame\n",
    "num_rows = 10**6  # 1 million rows\n",
    "df = pd.DataFrame({\n",
    "    'A': np.random.rand(num_rows),\n",
    "    'B': np.random.randint(1, 100, size=num_rows),\n",
    "    'C': np.random.choice(['X', 'Y', 'Z'], size=num_rows)\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "csv_file_path = 'large_file.csv'\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0add448a",
   "metadata": {},
   "source": [
    "### Step 2: Reading the CSV File Without Chunking\n",
    "\n",
    "Now, we will read the entire CSV file into a DataFrame without using chunking and measure the execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b56dd2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage without chunking: 102.02 MiB\n"
     ]
    }
   ],
   "source": [
    "def read_without_chunking():\n",
    "    # Measure time for reading without chunking\n",
    "    start_time = time.time()\n",
    "    df_full = pd.read_csv(csv_file_path)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return df_full, end_time - start_time\n",
    "\n",
    "# Measure memory usage\n",
    "mem_usage_no_chunking = memory_usage(read_without_chunking)\n",
    "print(f\"Memory usage without chunking: {max(mem_usage_no_chunking) - min(mem_usage_no_chunking):.2f} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed94930",
   "metadata": {},
   "source": [
    "### Step 3: Reading the CSV File With Chunking\n",
    "\n",
    "Next, we will read the same CSV file using chunking and measure the execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "acf9a260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage with chunking: 35.65 MiB\n"
     ]
    }
   ],
   "source": [
    "def read_with_chunking():\n",
    "    # Measure time for reading with chunking\n",
    "    start_time = time.time()\n",
    "    chunks = []\n",
    "    chunk_size = 100000  # Adjust chunk size as needed\n",
    "\n",
    "    for chunk in pd.read_csv(csv_file_path, chunksize=chunk_size):\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    df_chunked = pd.concat(chunks, ignore_index=True)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return df_chunked, end_time - start_time\n",
    "\n",
    "# Measure memory usage\n",
    "mem_usage_with_chunking = memory_usage(read_with_chunking)\n",
    "print(f\"Memory usage with chunking: {max(mem_usage_with_chunking) - min(mem_usage_with_chunking):.2f} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfea5fbd",
   "metadata": {},
   "source": [
    "### Summary of Results\n",
    "\n",
    "1. **Performance Comparison:** In many cases, reading data in chunks can be as fast as reading the entire file at once, especially if the data is large. Chunking can also help manage memory usage, preventing potential crashes due to memory overload.\n",
    "\n",
    "2. **Flexibility:** Chunking allows for the processing of large datasets without needing to load the entire dataset into memory, making it a valuable technique for data processing in real-world applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
